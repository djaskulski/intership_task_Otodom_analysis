{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d49381-c1ed-492c-a243-2a01bf118083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please send it back to me: data/new_data_replies.csv  -->  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568bd61d-5509-4c55-8c9f-7e3366264e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8dd432-4808-46d5-914b-db6af420c89c",
   "metadata": {},
   "source": [
    "# DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370ffbc8-7736-413b-8834-8856cedbcb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_otodom():\n",
    "    \"\"\"Load csv files with defined column names\"\"\"\n",
    "    \n",
    "    data_ads_cols = [\"date\", \"user_id\", \"ad_id\", \"category_id\", \"params\"]\n",
    "    data_replies_cols = [\"date\", \"user_id\", \"ad_id\", \"mails\", \"phones\"]\n",
    "    data_segmentation_cols = [\"user_id\", \"segment\"]\n",
    "    data_categories_cols = [\"category_id\", \"category_name\"]\n",
    "\n",
    "    # here you can find information about the announcements\n",
    "    data_ads_df = pd.read_csv(\"data/data_ads.csv\", delimiter=\";\", names=data_ads_cols)\n",
    "    # information about the response per advertisement per day\n",
    "    data_replies_df = pd.read_csv(\"data/data_replies.csv\", delimiter=\";\", names=data_replies_cols)\n",
    "    # segmentation mapping for each user\n",
    "    data_segments_df = pd.read_csv(\"data/data_segments.csv\", delimiter=\";\", names=data_segmentation_cols)\n",
    "    # mapping to category tree\n",
    "    data_categories_df = pd.read_csv(\"data/data_categories.csv\", delimiter=\";\", names=data_categories_cols)\n",
    "    \n",
    "    return [data_ads_df, data_replies_df, data_segments_df, data_categories_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd1a6bd-72a5-48a1-a7a9-175a783b6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_missing(source, column):\n",
    "    \"\"\"Cut rows with missing values from original source and make new df with only null values\"\"\"\n",
    "    \n",
    "    # list of null indicies\n",
    "    null_indices = source[source[\"phones\"].isnull()].index.tolist()\n",
    "\n",
    "    # copping nulls\n",
    "    null_list = []\n",
    "    for i in null_indices:\n",
    "           null_list.append(source.iloc[i])\n",
    "\n",
    "    # dropping nulls\n",
    "    not_null_replies = source.drop(null_indices)\n",
    "    not_null_replies.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # new DataFrame with missing values\n",
    "    data_replies_cols = [\"date\", \"user_id\", \"ad_id\", \"mails\", \"phones\"]\n",
    "    null_replies = pd.DataFrame(null_list, columns=data_replies_cols)\n",
    "    null_replies.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return [null_replies, not_null_replies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "472d57c6-b750-419e-8812-a7da5913984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing(source, df_names):\n",
    "    \"\"\"Check percent of missing values in your columns\"\"\"\n",
    "    \n",
    "    for df, names in zip(source, df_names):\n",
    "        print(f\"Missing in {names} %\\n\",round(df.isnull().sum()/len(df)*100, 2),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68f618f-9819-4c03-bb71-1f253c906c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_split(source):\n",
    "    \"\"\"Select features and target, then split it to train test\"\"\"\n",
    "    \n",
    "    # features\n",
    "    X = source.iloc[:,0:4]\n",
    "    # target\n",
    "    y = source.iloc[:,4]\n",
    "    \n",
    "    # split into train=0.8, test=0.2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    \n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ad914de-6684-4e63-b293-b19740ec8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_pipe_model(X_train, X_test, y_train, y_test, to_pred):\n",
    "    \"\"\"Pipeline with grid search cv\"\"\"\n",
    "    \n",
    "    # feature's data\n",
    "    to_pred = to_pred.drop([\"phones\"], axis=1)\n",
    "    splits = [X_train, X_test, to_pred]\n",
    "    \n",
    "    # convert string data to datatime and, then convert to ordinal\n",
    "    for split in splits:\n",
    "        split[\"date\"] = pd.to_datetime(split[\"date\"])\n",
    "        split[\"date\"] = split[\"date\"].apply(lambda x: x.toordinal())\n",
    "\n",
    "    # pipeline\n",
    "    pipe = Pipeline([(\"classifier\", KNeighborsClassifier())])\n",
    "    \n",
    "    # search space with models and parameters\n",
    "    search_space = [\n",
    "    {\"classifier\": [KNeighborsClassifier()],\n",
    "    \"classifier__n_neighbors\": np.arange(5, 35, 5),\n",
    "    \"classifier__leaf_size\": np.arange(5, 35, 5),\n",
    "    \"classifier__p\": [1, 2]},\n",
    "    \n",
    "    {\"classifier\": [DecisionTreeClassifier()],\n",
    "    \"classifier__max_depth\": [3, None],\n",
    "    \"classifier__min_samples_leaf\": np.arange(2, 12, 2),\n",
    "    \"classifier__max_features\": np.arange(1, 4, 1),\n",
    "    \"classifier__criterion\": [\"gini\", \"entropy\"]},\n",
    "        \n",
    "    {\"classifier\": [SVC()],\n",
    "    \"classifier__kernel\": ['linear', 'rbf', 'poly'],\n",
    "    \"classifier__C\": np.logspace(0, 4, num=10),\n",
    "    \"classifier__gamma\": np.linspace(0.1, 100, num=10),\n",
    "    \"classifier__degree\": np.arange(0, 7, 1)}\n",
    "]\n",
    "    \n",
    "    # standar scaling\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_std = scaler.transform(X_train)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "    X_pred_std = scaler.transform(to_pred)\n",
    "\n",
    "    # gridsearch, fit, predict\n",
    "    gridsearch = GridSearchCV(pipe, search_space, cv=4, verbose=1, n_jobs=-1)\n",
    "    best_model = gridsearch.fit(X_train_std, y_train)\n",
    "    preds = best_model.predict(X_pred_std)\n",
    "    \n",
    "    # scores\n",
    "    allscores = model.cv_results_\n",
    "    print(allscores)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b30c674f-ad0d-4c88-b56c-1eac8ff1ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_replies(source_1, source_2, pred):\n",
    "    \"\"\"Insert predicted values into null replies and join it with not null replies\"\"\"\n",
    "    \n",
    "    replies_1 = source_1\n",
    "    replies_2 = source_2\n",
    "    replies_2[\"phones\"] = pred\n",
    "    \n",
    "    # join DataFrames in axis=0\n",
    "    data_replies = replies_1.append(replies_2, ignore_index=True)\n",
    "    \n",
    "    return data_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e0f5d34-d510-4ef7-993e-ecbd5ca15996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_job():\n",
    "    # List of OLX DataFrames\n",
    "    data_ads_df, data_replies_df, data_segments_df, data_categories_df = load_otodom()\n",
    "    # Returns: [data_ads_df, data_replies_df, data_segments_df, data_categories_df]\n",
    "    \n",
    "    olx_tables_data = [data_ads_df, data_replies_df, data_segments_df, data_categories_df]\n",
    "    olx_tables_names = [\"data_ads_df\", \"data_replies_df\", \"data_segments_df\", \"data_categories_df\"]\n",
    "    \n",
    "    # Check percent of missing values \n",
    "    check_missing(source=olx_tables_data, df_names=olx_tables_names)\n",
    "    # Returns: None    \n",
    "\n",
    "    # Split original source into null/not null replies\n",
    "    null_replies, not_null_replies = cut_missing(source=data_replies_df, column=\"phones\")\n",
    "    # Returns: [null_replies, not_null_replies]\n",
    "\n",
    "    # Try to load files from csv or make new\n",
    "    try:\n",
    "        new_data_replies = pd.read_csv(\"data/new_data_replies.csv\")\n",
    "    except Exception as e:\n",
    "           print(\"Error has occurred: \", e, \"\\n\")\n",
    "    finally:\n",
    "        # Select features, target and split it\n",
    "        X_train, X_test, y_train, y_test = select_split(source=not_null_replies)\n",
    "        # Retruns: [X_train, X_test, y_train, y_test]\n",
    "\n",
    "        train_test_data = [X_train, X_test, y_train, y_test]\n",
    "        train_test_names = [\"X_train\", \"X_test\", \"y_train\", \"y_test\"]\n",
    "        \n",
    "        # Check percent of missing values \n",
    "        check_missing(source=train_test_data, df_names=train_test_names)\n",
    "        # Returns: None\n",
    "\n",
    "        # Find best classification estimator and predict null values\n",
    "        preds = best_pipe_model(X_train, X_test, y_train, y_test, to_pred=null_replies)\n",
    "        # Returns: preds\n",
    "        \n",
    "        # Compound replies\n",
    "        new_data_replies = join_replies(source_1=not_null_replies, source_2=null_replies, pred=preds)\n",
    "        # Returns: data_replies\n",
    "\n",
    "        # Update data_replies_df \n",
    "        data_replies_df = new_data_replies\n",
    "        data_replies_df.to_csv(\"data/new_data_replies.csv\")\n",
    "\n",
    "    return [data_ads_df, data_replies_df, data_segments_df, data_categories_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2a5c6-f646-45e7-85ca-2fbec8e83367",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c2f5f8b-46a2-4181-884a-fe3c28d25082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_connect(db_file):\n",
    "    \"\"\"Establish connection with local database\"\"\"\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(\"Connected to sqlite3 ver: \", sqlite3.version)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d22b7a57-a2be-4fce-a9ce-b1fdcf739e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_insert(source, tables, connection):\n",
    "    \"\"\"Insert data to the database from DataFrames\"\"\"\n",
    "    \n",
    "    # [data_ads_df, data_replies_df, data_segments_df, data_categories_df]\n",
    "    indices = [0, 1, 2, 3]\n",
    "    \n",
    "    for i, tab in zip(indices, tables):\n",
    "        source[i].to_sql(tab, connection, if_exists='replace', index=False)\n",
    "    print(\"Data inserted. Tables successfully made\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10dabcf9-a48b-407a-bbcc-cf81ec2c05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_query(connection, query):\n",
    "    \"\"\"Send request to db and return response in DataFrame\"\"\"\n",
    "    \n",
    "    response_df = pd.read_sql_query(query, connection)\n",
    "    return response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5243b75b-003a-4abe-9728-59eb35428391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_job(source):\n",
    "    \n",
    "    database = r\"otodom.db\"\n",
    "    table_list = [\"ads\", \"replies\", \"segments\", \"categories\"]\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3_connect(database)\n",
    "    # Retruns: conn\n",
    "    \n",
    "    #zwraca ile ogłoszen dodał user w ciągu ostatnich 7 dni\n",
    "    sql_my_query_1 =\"\"\"\n",
    "    select user_id, count(distinct ad_id)\n",
    "    from replies\n",
    "    where date <= '2019-04-30' and date >= '2019-04-24'\n",
    "    group by user_id\n",
    "    order by user_id;\n",
    "    \"\"\"\n",
    "\n",
    "    #zwraca odpowiedzi tel oraz mail dla usera na każdy dzień z osobna\n",
    "    sql_my_query_2 =\"\"\"\n",
    "    select date, user_id, sum(phones), sum(mails)\n",
    "    from replies\n",
    "    where date <= '2019-04-30' and date >= '2019-04-24'\n",
    "    group by date, user_id\n",
    "    order by date, user_id;\n",
    "    \"\"\"\n",
    "\n",
    "    #zwraca ogólną ilość odpowiedzi tel oraz mail dla danego użytkownika w przedziale 7 dni\n",
    "    sql_my_query_3 = \"\"\"\n",
    "    select user_id, sum(phones), sum(mails)\n",
    "    from replies\n",
    "    where date <= '2019-04-30' and date >= '2019-04-24'\n",
    "    group by user_id\n",
    "    order by user_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    if conn is not None:\n",
    "        # Insert csv to sqlite3 database\n",
    "        sqlite3_insert(source=source, tables=table_list, connection=conn)\n",
    "        # Returns: None\n",
    "        \n",
    "        # Enquire db\n",
    "        query_1 = sqlite3_query(connection=conn, query=sql_my_query_1)\n",
    "        query_1 = sqlite3_query(connection=conn, query=sql_my_query_2)\n",
    "        query_1 = sqlite3_query(connection=conn, query=sql_my_query_3)\n",
    "        # Returns: response_df\n",
    "        print(\"SQL job is done.\")\n",
    "    else:\n",
    "        print(\"Error! cannot create the database connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dca0a7-4d84-400f-8e3d-0033d58621e7",
   "metadata": {},
   "source": [
    "# LIQUIDITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c0dc425-38b5-4455-9e44-3945956cee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liquidity_per_user():\n",
    "    \"\"\"\n",
    "    Liquidity will be understood as % of advertisements which have received \n",
    "    at least 1 response (by phone or e-mail) within a period of 7 days \n",
    "    (including day 0 - the day of adding an day of adding an ad)\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce8d80e-2ea7-4ffe-8cd5-5cf79aaa44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data_analysis():\n",
    "    \"\"\" \n",
    "    Jupyter/R Markdown preferred for analysis\n",
    "\n",
    "    Scripts can be in separate files, or as part of a notebook depending on\n",
    "    selected methods\n",
    "\n",
    "    Please present your final results and most important conclusions in the \n",
    "    form of a presentation (e.g. Google slides)\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6d17435-9c23-4336-8485-bd0b4b36e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_1():\n",
    "    \"\"\" \n",
    "    What differences do you see between the segments in terms of the data \n",
    "    you have available (including liquidity)?\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa1420e8-58e7-4bba-b09c-d83e11f56002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_2():\n",
    "    \"\"\"What do you think might influence higher or lower levels of liquidity?\"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a8c24-5610-4e07-b92e-eacd1863dc66",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b03ed3cc-4ef6-41b8-a465-e1f168a06583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Data processing job, datatime and null values\n",
    "    data = dp_job()\n",
    "    \n",
    "    # SQL job, make db, insert data and send requests\n",
    "    sql_job(source=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f469aad-dd2d-419e-89d7-a76c89050590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error has occurred:  [Errno 2] No such file or directory: 'data/new_data_replies.csv' \n",
      "\n",
      "Fitting 4 folds for each of 2232 candidates, totalling 8928 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DJaskulski\\miniconda3\\envs\\olx_otodom\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 11.5min\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    %time main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a2ba3-dcb8-4d1e-adf4-a736e9e03632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:olx_otodom] *",
   "language": "python",
   "name": "conda-env-olx_otodom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
