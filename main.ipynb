{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "568bd61d-5509-4c55-8c9f-7e3366264e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8dd432-4808-46d5-914b-db6af420c89c",
   "metadata": {},
   "source": [
    "# DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "370ffbc8-7736-413b-8834-8856cedbcb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_otodom():\n",
    "    \"\"\"Load csv files with defined column names\"\"\"\n",
    "    \n",
    "    data_ads_cols = [\"date\", \"user_id\", \"ad_id\", \"category_id\", \"params\"]\n",
    "    data_replies_cols = [\"date\", \"user_id\", \"ad_id\", \"mails\", \"phones\"]\n",
    "    data_segmentation_cols = [\"user_id\", \"segment\"]\n",
    "    data_categories_cols = [\"category_id\", \"category_name\"]\n",
    "\n",
    "    # here you can find information about the announcements\n",
    "    data_ads_df = pd.read_csv(\"data/data_ads.csv\", delimiter=\";\", names=data_ads_cols)\n",
    "    # information about the response per advertisement per day\n",
    "    data_replies_df = pd.read_csv(\"data/data_replies.csv\", delimiter=\";\", names=data_replies_cols)\n",
    "    # segmentation mapping for each user\n",
    "    data_segments_df = pd.read_csv(\"data/data_segments.csv\", delimiter=\";\", names=data_segmentation_cols)\n",
    "    # mapping to category tree\n",
    "    data_categories_df = pd.read_csv(\"data/data_categories.csv\", delimiter=\";\", names=data_categories_cols)\n",
    "    \n",
    "    return [data_ads_df, data_replies_df, data_segments_df, data_categories_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8fd1a6bd-72a5-48a1-a7a9-175a783b6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_missing(source, column):\n",
    "    \"\"\"Cut rows with missing values from original source and make new df with only null values\"\"\"\n",
    "    \n",
    "    # list of null indicies\n",
    "    null_indices = source[source[\"phones\"].isnull()].index.tolist()\n",
    "\n",
    "    # copping nulls\n",
    "    null_list = []\n",
    "    for i in null_indices:\n",
    "           null_list.append(source.iloc[i])\n",
    "\n",
    "    # dropping nulls\n",
    "    not_null_replies = source.drop(null_indices)\n",
    "    not_null_replies.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # new DataFrame with missing values\n",
    "    data_replies_cols = [\"date\", \"user_id\", \"ad_id\", \"mails\", \"phones\"]\n",
    "    null_replies = pd.DataFrame(null_list, columns=data_replies_cols)\n",
    "    null_replies.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(\"data_replies_df splitted into: null_replies, not_null_replies\\n\")\n",
    "    \n",
    "    return [null_replies, not_null_replies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "472d57c6-b750-419e-8812-a7da5913984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing(source, df_names):\n",
    "    \"\"\"Check percent of missing values in your columns\"\"\"\n",
    "    \n",
    "    for df, names in zip(source, df_names):\n",
    "        print(f\"Missing in {names} %\\n\",round(df.isnull().sum()/len(df)*100, 2),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a68f618f-9819-4c03-bb71-1f253c906c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_split(source):\n",
    "    \"\"\"Select features and target, then split it to train test\"\"\"\n",
    "    \n",
    "    # features\n",
    "    X = source.iloc[:,0:4]\n",
    "    # target\n",
    "    y = source.iloc[:,4]\n",
    "    \n",
    "    # split into train=0.8, test=0.2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    print(\"not_null_replies splitted into: X_train, X_test, y_train, y_test\\n\")\n",
    "    \n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ad914de-6684-4e63-b293-b19740ec8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_pipe_model(X_train, X_test, y_train, y_test, to_pred):\n",
    "    \"\"\"Pipeline with grid search cv\"\"\"\n",
    "    \n",
    "    # feature's data\n",
    "    to_pred = to_pred.drop([\"phones\"], axis=1)\n",
    "    splits = [X_train, X_test, to_pred]\n",
    "    \n",
    "    # convert string data to datatime and, then convert to ordinal\n",
    "    for split in splits:\n",
    "        split[\"date\"] = pd.to_datetime(split[\"date\"])\n",
    "        split[\"date\"] = split[\"date\"].apply(lambda x: x.toordinal())\n",
    "    \n",
    "    # pipeline\n",
    "    pipe = Pipeline([(\"regressor\", LinearRegression())])\n",
    "    \n",
    "    # search space with models and parameters\n",
    "    search_space = [\n",
    "    {\"regressor\": [LinearRegression()],\n",
    "    \"regressor__normalize\": [False, True]},\n",
    "        \n",
    "    {\"regressor\": [Ridge()],\n",
    "    \"regressor__alpha\": np.linspace(0, 0.5, 21),\n",
    "    \"regressor__max_iter\": [1000, None]},\n",
    "    \n",
    "    {\"regressor\": [Lasso()],\n",
    "    \"regressor__alpha\": np.linspace(0, 0.2, 21),\n",
    "    \"regressor__max_iter\": [1000, None]}\n",
    "    ]\n",
    "\n",
    "    # standar scaling\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_std = scaler.transform(X_train)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "    X_pred_std = scaler.transform(to_pred)\n",
    "\n",
    "    # gridsearch, fit, predict\n",
    "    gridsearch = GridSearchCV(pipe, search_space, cv=4, verbose=1, n_jobs=-1)\n",
    "    best_model = gridsearch.fit(X_train_std, y_train)\n",
    "    preds = best_model.predict(X_pred_std)\n",
    "    \n",
    "    # round to int\n",
    "    rounded_preds = [round(elem) for elem in preds]\n",
    "    \n",
    "    # scores\n",
    "    best_score = gridsearch.best_score_\n",
    "    print(\"best_score: \", best_score)\n",
    "    \n",
    "    return rounded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b30c674f-ad0d-4c88-b56c-1eac8ff1ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_replies(source_1, source_2, pred):\n",
    "    \"\"\"Insert predicted values into null replies and join it with not null replies\"\"\"\n",
    "    \n",
    "    replies_1 = source_1\n",
    "    replies_2 = source_2\n",
    "    replies_2[\"phones\"] = pred\n",
    "    \n",
    "    # join DataFrames in axis=0\n",
    "    data_replies = replies_1.append(replies_2, ignore_index=True)\n",
    "    \n",
    "    return data_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2e0f5d34-d510-4ef7-993e-ecbd5ca15996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_job():\n",
    "    # List of OLX DataFrames\n",
    "    data_ads_df, data_replies_df, data_segments_df, data_categories_df = load_otodom()\n",
    "    # Returns: [data_ads_df, data_replies_df, data_segments_df, data_categories_df]\n",
    "    \n",
    "    olx_tables_data = [data_ads_df, data_replies_df, data_segments_df, data_categories_df]\n",
    "    olx_tables_names = [\"data_ads_df\", \"data_replies_df\", \"data_segments_df\", \"data_categories_df\"]\n",
    "    \n",
    "    # Check percent of missing values \n",
    "    check_missing(source=olx_tables_data, df_names=olx_tables_names)\n",
    "    # Returns: None    \n",
    "\n",
    "    # Split original source into null/not null replies\n",
    "    null_replies, not_null_replies = cut_missing(source=data_replies_df, column=\"phones\")\n",
    "    # Returns: [null_replies, not_null_replies]\n",
    "\n",
    "    # Try to load files from csv or make new\n",
    "    try:\n",
    "        new_data_replies = pd.read_csv(\"data/new_data_replies.csv\")\n",
    "    except Exception as e:\n",
    "           print(\"Error has occurred: \", e, \"\\n\")\n",
    "    finally:\n",
    "        # Select features, target and split it\n",
    "        X_train, X_test, y_train, y_test = select_split(source=not_null_replies)\n",
    "        # Retruns: [X_train, X_test, y_train, y_test]\n",
    "\n",
    "        train_test_data = [X_train, X_test, y_train, y_test]\n",
    "        train_test_names = [\"X_train\", \"X_test\", \"y_train\", \"y_test\"]\n",
    "        \n",
    "        # Check percent of missing values \n",
    "        check_missing(source=train_test_data, df_names=train_test_names)\n",
    "        # Returns: None\n",
    "\n",
    "        # Find best classification estimator and predict null values\n",
    "        preds = best_pipe_model(X_train, X_test, y_train, y_test, to_pred=null_replies)\n",
    "        # Returns: preds\n",
    "        \n",
    "        # Compound replies\n",
    "        new_data_replies = join_replies(source_1=not_null_replies, source_2=null_replies, pred=preds)\n",
    "        # Returns: data_replies\n",
    "\n",
    "        # Update data_replies_df \n",
    "        data_replies_df = new_data_replies\n",
    "        data_replies_df.to_csv(\"data/new_data_replies.csv\")\n",
    "\n",
    "    return [data_ads_df, data_replies_df, data_segments_df, data_categories_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2a5c6-f646-45e7-85ca-2fbec8e83367",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c2f5f8b-46a2-4181-884a-fe3c28d25082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_connect(db_file):\n",
    "    \"\"\"Establish connection with local database\"\"\"\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(\"Connected to sqlite3 ver: \", sqlite3.version)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d22b7a57-a2be-4fce-a9ce-b1fdcf739e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_insert(source, tables, connection):\n",
    "    \"\"\"Insert data to the database from DataFrames\"\"\"\n",
    "    \n",
    "    # [data_ads_df, data_replies_df, data_segments_df, data_categories_df]\n",
    "    indices = [0, 1, 2, 3]\n",
    "    \n",
    "    for i, tab in zip(indices, tables):\n",
    "        source[i].to_sql(tab, connection, if_exists='replace', index=False)\n",
    "    print(\"Data inserted. Tables successfully made\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "10dabcf9-a48b-407a-bbcc-cf81ec2c05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_query_1(connection):\n",
    "    \"\"\"Send request to db and return response in DataFrame\"\"\"\n",
    "    \n",
    "    # Returns number of ads for user_id in last 7 days\n",
    "    query = \"\"\"\n",
    "    select user_id, count(distinct ad_id)\n",
    "    from replies\n",
    "    where date <= '2019-04-30' and date >= '2019-04-24\n",
    "    group by user_id\n",
    "    order by user_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    response_df = pd.read_sql_query(query, connection)\n",
    "    return response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ce75736d-e07d-4ce0-95a1-85386e82afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_query_2(connection):\n",
    "    \"\"\"Send request to db and return response in DataFrame\"\"\"\n",
    "    \n",
    "    # Returns the number of answers by phone and email for a user for each day separately\n",
    "    query = \"\"\"\n",
    "    select date, user_id, sum(phones), sum(mails)\n",
    "    from replies\n",
    "    where date <= '2019-04-30' and date >= '2019-04-24'\n",
    "    group by date, user_id\n",
    "    order by date, user_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    response_df = pd.read_sql_query(query, connection)\n",
    "    return response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c68efb25-eb1e-47bc-94c9-9ec0f547a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_query_3(connection):\n",
    "    \"\"\"Send request to db and return response in DataFrame\"\"\"\n",
    "    \n",
    "    # Returns the total number of responses by phone and email for a given user within a 7-day period\n",
    "    query = \"\"\"\n",
    "    select user_id, sum(phones), sum(mails)\n",
    "    from replies\n",
    "    where date <= '2019-04-30' and date >= '2019-04-24'\n",
    "    group by user_id\n",
    "    order by user_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    response_df = pd.read_sql_query(query, connection)\n",
    "    return response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5243b75b-003a-4abe-9728-59eb35428391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_job(source):\n",
    "    \n",
    "    database = r\"otodom.db\"\n",
    "    table_list = [\"ads\", \"replies\", \"segments\", \"categories\"]\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3_connect(database)\n",
    "    # Retruns: conn\n",
    "    \n",
    "    if conn is not None:\n",
    "        # Insert csv to sqlite3 database\n",
    "        sqlite3_insert(source=source, tables=table_list, connection=conn)\n",
    "        # Returns: None\n",
    "        \n",
    "        # Enquire db\n",
    "        query_1 = sqlite3_query_1(connection=conn)\n",
    "        query_2 = sqlite3_query_2(connection=conn)\n",
    "        query_3 = sqlite3_query_3(connection=conn)\n",
    "        # Returns: response_df\n",
    "        print(\"SQL job is done.\")\n",
    "    else:\n",
    "        print(\"Error! cannot create the database connection.\")\n",
    "        \n",
    "    return [query_1, query_2, query_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dca0a7-4d84-400f-8e3d-0033d58621e7",
   "metadata": {},
   "source": [
    "# LIQUIDITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8c0dc425-38b5-4455-9e44-3945956cee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liquidity_per_user():\n",
    "    \"\"\"\n",
    "    Liquidity will be understood as % of advertisements which have received \n",
    "    at least 1 response (by phone or e-mail) within a period of 7 days \n",
    "    (including day 0 - the day of adding an day of adding an ad)\n",
    "    \"\"\"\n",
    "    my_answer = \"Automation of sql queries to database\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bce8d80e-2ea7-4ffe-8cd5-5cf79aaa44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data_analysis():\n",
    "    \"\"\" \n",
    "    Jupyter/R Markdown preferred for analysis\n",
    "\n",
    "    Scripts can be in separate files, or as part of a notebook depending on\n",
    "    selected methods\n",
    "\n",
    "    Please present your final results and most important conclusions in the \n",
    "    form of a presentation (e.g. Google slides)\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b6d17435-9c23-4336-8485-bd0b4b36e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_1():\n",
    "    \"\"\" \n",
    "    What differences do you see between the segments in terms of the data \n",
    "    you have available (including liquidity)?\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fa1420e8-58e7-4bba-b09c-d83e11f56002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_2():\n",
    "    \"\"\"What do you think might influence higher or lower levels of liquidity?\"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a8c24-5610-4e07-b92e-eacd1863dc66",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b03ed3cc-4ef6-41b8-a465-e1f168a06583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Data processing job, datatime and null values\n",
    "    data = dp_job()\n",
    "    \n",
    "    # SQL job, make db, insert data and send requests\n",
    "    query_df1, query_df2, query_df2 = sql_job(source=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1f469aad-dd2d-419e-89d7-a76c89050590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing in data_ads_df %\n",
      " date           0.0\n",
      "user_id        0.0\n",
      "ad_id          0.0\n",
      "category_id    0.0\n",
      "params         0.0\n",
      "dtype: float64 \n",
      "\n",
      "Missing in data_replies_df %\n",
      " date        0.00\n",
      "user_id     0.00\n",
      "ad_id       0.00\n",
      "mails       0.00\n",
      "phones     14.64\n",
      "dtype: float64 \n",
      "\n",
      "Missing in data_segments_df %\n",
      " user_id    0.0\n",
      "segment    0.0\n",
      "dtype: float64 \n",
      "\n",
      "Missing in data_categories_df %\n",
      " category_id      0.0\n",
      "category_name    0.0\n",
      "dtype: float64 \n",
      "\n",
      "data_replies_df splitted into: null_replies, not_null_replies\n",
      "\n",
      "not_null_replies splitted into: X_train, X_test, y_train, y_test\n",
      "\n",
      "Missing in X_train %\n",
      " date       0.0\n",
      "user_id    0.0\n",
      "ad_id      0.0\n",
      "mails      0.0\n",
      "dtype: float64 \n",
      "\n",
      "Missing in X_test %\n",
      " date       0.0\n",
      "user_id    0.0\n",
      "ad_id      0.0\n",
      "mails      0.0\n",
      "dtype: float64 \n",
      "\n",
      "Missing in y_train %\n",
      " 0.0 \n",
      "\n",
      "Missing in y_test %\n",
      " 0.0 \n",
      "\n",
      "Fitting 4 folds for each of 86 candidates, totalling 344 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 344 | elapsed:   26.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score:  0.0015228537409755771\n",
      "Connected to sqlite3 ver:  2.6.0\n",
      "Data inserted. Tables successfully made\n",
      "SQL job is done.\n",
      "Wall time: 53.6 s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    %time main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:olx_otodom] *",
   "language": "python",
   "name": "conda-env-olx_otodom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
