{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "568bd61d-5509-4c55-8c9f-7e3366264e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8dd432-4808-46d5-914b-db6af420c89c",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "370ffbc8-7736-413b-8834-8856cedbcb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_otodom():\n",
    "    \"\"\"Load csv files with defined column names\"\"\"\n",
    "    \n",
    "    data_ads_cols = [\"date\", \"user_id\", \"ad_id\", \"category_id\", \"params\"]\n",
    "    data_replies_cols = [\"date\", \"user_id\", \"ad_id\", \"mails\", \"phones\"]\n",
    "    data_segmentation_cols = [\"user_id\", \"segment\"]\n",
    "    data_categories_cols = [\"category_id\", \"category_name\"]\n",
    "\n",
    "    # here you can find information about the announcements\n",
    "    data_ads_df = pd.read_csv(\"data/data_ads.csv\", delimiter=\";\", names=data_ads_cols)\n",
    "    # information about the response per advertisement per day\n",
    "    data_replies_df = pd.read_csv(\"data/data_replies.csv\", delimiter=\";\", names=data_replies_cols)\n",
    "    # segmentation mapping for each user\n",
    "    data_segments_df = pd.read_csv(\"data/data_segments.csv\", delimiter=\";\", names=data_segmentation_cols)\n",
    "    # mapping to category tree\n",
    "    data_categories_df = pd.read_csv(\"data/data_categories.csv\", delimiter=\";\", names=data_categories_cols)\n",
    "    \n",
    "    return [data_ads_df, data_replies_df, data_segments_df, data_categories_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "7b6d777b-f117-4a04-bbb0-0799ad788c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_info(source):\n",
    "    \"\"\"Check columns type for each DataFrame\"\"\"\n",
    "    \n",
    "    print(\"Checking info: \\n\")\n",
    "    \n",
    "    for df in source:\n",
    "        print (df.info(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "8fd1a6bd-72a5-48a1-a7a9-175a783b6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_missing(source, column):\n",
    "    \"\"\"Cut rows with missing values from original source and make new df with only null values\"\"\"\n",
    "    \n",
    "    replies = source\n",
    "    null_array = np.array([]) # Im not sure if its works.\n",
    "    null_indices =  replies[replies[column].isnull()].index.tolist()\n",
    "    \n",
    "    # cutting nulls\n",
    "    for i in null_indices:\n",
    "        np.append(null_array, replies.loc[i])\n",
    "    \n",
    "    # dropping nulls\n",
    "    for i in null_indices:\n",
    "        replies.drop(replies.index[i], inplace=True)\n",
    "    \n",
    "    not_null_replies = replies\n",
    "    \n",
    "    # new DataFrame with missing values\n",
    "    null_replies = pd.DataFrame(null_array)\n",
    "    \n",
    "    # saving to csv\n",
    "    null_replies.to_csv(\"data/null_replies.csv\")\n",
    "    not_null_replies.to_csv(\"data/not_null_replies.csv\")\n",
    "    \n",
    "    return [null_replies, not_null_replies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716ba97-617f-4df1-b718-430c0e6b1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_replies():\n",
    "    \"\"\"Load csv files with null/not null replies\"\"\"\n",
    "    \n",
    "    #data_replies_cols = [\"date\", \"user_id\", \"ad_id\", \"mails\", \"phones\"]\n",
    "    \n",
    "    # segmentation mapping for each user\n",
    "    null_replies = pd.read_csv(\"data/null_replies.csv\"#, names=data_replies_cols)\n",
    "    # mapping to category tree\n",
    "    not_null_replies = pd.read_csv(\"data/not_null_replies.csv\"#, names=data_replies_cols)\n",
    "    \n",
    "    return [null_replies, not_null_replies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "a68f618f-9819-4c03-bb71-1f253c906c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_split(source):\n",
    "    \"\"\" \"\"\"\n",
    "    \n",
    "    # features\n",
    "    X = source.iloc[:,0:4]\n",
    "    # target\n",
    "    y = source.iloc[:,4]\n",
    "    \n",
    "    # split into train=0.8, test=0.2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    \n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "472d57c6-b750-419e-8812-a7da5913984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing(df_names, *args):\n",
    "    \"\"\" \"\"\"\n",
    "    my_list = [*args]\n",
    "    for df, names in zip(my_list, df_names):\n",
    "        print(f\"Missing in {names} %\\n\",round(df.isnull().sum()/len(df)*100, 2),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "7ad914de-6684-4e63-b293-b19740ec8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_linear_regression(*args):\n",
    "    \"\"\" \"\"\"\n",
    "    # linear regression\n",
    "    logistic = LogisticRegression(solver=\"liblinear\")\n",
    "    penalty = [\"l1\", \"l2\"]\n",
    "    C = np.logspace(0, 4, 1000)\n",
    "    hyperparameters = dict(C=C, penalty=penalty)\n",
    "    \n",
    "    # standard scaling\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_std = scaler.transform(X_train)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "    \n",
    "    # randomized search\n",
    "    randomizedsearch = RandomizedSearchCV(\n",
    "        logistic,\n",
    "        hyperparameters,\n",
    "        random_state=1,\n",
    "        n_iter=1000,\n",
    "        cv=5,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    best_random_model = randomizedsearch.fit(X_train_std, y_train)\n",
    "    preds = randomizedsearch.predict(X_test_std)\n",
    "     \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "b30c674f-ad0d-4c88-b56c-1eac8ff1ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_replies(source_1, source_2, pred):\n",
    "    \"\"\" \"\"\"\n",
    "    \n",
    "    # join\n",
    "    replies_1 = source_1\n",
    "    replies_2 = source_2[\"phones\"] = pred\n",
    "    data_replies = replies_1.append(replies_2, ignore_index=True)\n",
    "    \n",
    "    return data_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f5d34-d510-4ef7-993e-ecbd5ca15996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_job():\n",
    "    # List of OLX DataFrames\n",
    "    data = load_otodom()\n",
    "    # Returns: [data_ads_df, data_replies_df, data_segments_df, data_categories_df]\n",
    "    \n",
    "    # DataFrame: data_replies_df\n",
    "    data_replies_df = data[1]\n",
    "    \n",
    "    # Names of splitted DataFrames\n",
    "    split_names = [\"X_train\", \"X_test\", \"X_val\", \"y_train\", \"y_test\", \"y_val\"]\n",
    "    \n",
    "    # Check info\n",
    "    check_info(source=data)\n",
    "    # Returns: None\n",
    "    \n",
    "     # Try load files from csv or make new\n",
    "    try:\n",
    "        null_replies, not_null_replies = load_replies()\n",
    "    except Exception as e:\n",
    "           print(\"Error has occurred: \", e, \"\\n\")\n",
    "    else:\n",
    "        # Split original source into two pieces [null/not null] ans save it to csv\n",
    "        null_replies, not_null_replies = cut_missing(source=data_replies_df, column=\"phones\")\n",
    "        # Returns: [null_replies, not_null_replies]\n",
    "\n",
    "    # Select features, target and split it\n",
    "    X_train, X_test, y_train, y_test = select_split(source=not_null_replies)\n",
    "    # Retruns:  [X_train, X_test, y_train, y_test]\n",
    "    \n",
    "    # Check percent of missing values \n",
    "    check_missing(source=splitted, df_names=split_names)\n",
    "    # Returns: None\n",
    "    \n",
    "    # Classification WIP\n",
    "    preds = best_linear_regression(X_train, X_test, y_train, y_test)\n",
    "    # Returns: preds\n",
    "    \n",
    "    new_data_replies = join_replies(source_1=not_null_replies, source_2=null_replies, pred=preds)\n",
    "    # Returns data_replies\n",
    "    \n",
    "    # Update list of DataFrames\n",
    "    data.remove(data_replies_df)\n",
    "    data.append(new_data_replies)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2a5c6-f646-45e7-85ca-2fbec8e83367",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "0c2f5f8b-46a2-4181-884a-fe3c28d25082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_connect(db_file):\n",
    "    \"\"\"Establish connection with local database\"\"\"\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(\"Connected to sqlite3 ver: \", sqlite3.version)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "d22b7a57-a2be-4fce-a9ce-b1fdcf739e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_insert(source, tables, connection):\n",
    "    \"\"\"Insert data to the database from DataFrames\"\"\"\n",
    "    \n",
    "    # [data_ads_df, data_replies_df, data_segments_df, data_categories_df]\n",
    "    indices = [0, 1, 2, 3]\n",
    "    \n",
    "    for i, tab in zip(indices, tables):\n",
    "        source[i].to_sql(tab, connection, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "10dabcf9-a48b-407a-bbcc-cf81ec2c05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_query(connection, query):\n",
    "    \"\"\"Create a table from table_query statement\"\"\"\n",
    "    \n",
    "    try:\n",
    "        connection.execute(query)\n",
    "        print(\"Query send!\")\n",
    "    except Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "5243b75b-003a-4abe-9728-59eb35428391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_job(source):\n",
    "    \"\"\" \"\"\"\n",
    "    \n",
    "    database = r\"otodom.db\"\n",
    "    table_list = [\"ads\", \"replies\", \"segments\", \"categories\"]\n",
    "    \n",
    "    sql_my_query = \" \"\n",
    "    \n",
    "    conn = sqlite3_connect(database)\n",
    "    \n",
    "    if conn is not None:\n",
    "        sqlite3_insert(source=source, tables=table_list, connection=conn)\n",
    "        #sqlite3_query(connection=conn, query=sql_my_query)\n",
    "        #sqlite3_query(connection=conn, query=sql_my_query)\n",
    "        #sqlite3_query(connection=conn, query=sql_my_query)\n",
    "        print(\"SQL job is done.\")\n",
    "    else:\n",
    "        print(\"Error! cannot create the database connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dca0a7-4d84-400f-8e3d-0033d58621e7",
   "metadata": {},
   "source": [
    "# LIQUIDITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "8c0dc425-38b5-4455-9e44-3945956cee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liquidity_per_user():\n",
    "    \"\"\"\n",
    "    Liquidity will be understood as % of advertisements which have received \n",
    "    at least 1 response (by phone or e-mail) within a period of 7 days \n",
    "    (including day 0 - the day of adding an day of adding an ad)\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "bce8d80e-2ea7-4ffe-8cd5-5cf79aaa44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data_analysis():\n",
    "    \"\"\" \n",
    "    Jupyter/R Markdown preferred for analysis\n",
    "\n",
    "    Scripts can be in separate files, or as part of a notebook depending on\n",
    "    selected methods\n",
    "\n",
    "    Please present your final results and most important conclusions in the \n",
    "    form of a presentation (e.g. Google slides)\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "b6d17435-9c23-4336-8485-bd0b4b36e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_1():\n",
    "    \"\"\" \n",
    "    What differences do you see between the segments in terms of the data \n",
    "    you have available (including liquidity)?\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "fa1420e8-58e7-4bba-b09c-d83e11f56002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_2():\n",
    "    \"\"\"What do you think might influence higher or lower levels of liquidity?\"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a8c24-5610-4e07-b92e-eacd1863dc66",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "b03ed3cc-4ef6-41b8-a465-e1f168a06583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Predict missing values with LinearRegression\n",
    "    data = ml_job()\n",
    "    \n",
    "    # Make db, insert data from df\n",
    "    sql_job(source=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "1f469aad-dd2d-419e-89d7-a76c89050590",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 657032 is out of bounds for axis 0 with size 657029",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-372-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-371-d239d23af3c1>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcut_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"phones\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Returns: null_dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-358-ef8b6cc2447e>\u001b[0m in \u001b[0;36mcut_missing\u001b[1;34m(source, column)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# dropping nulls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnull_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# new DataFrame with missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\olx_otodom\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4295\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4296\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4297\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4299\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 657032 is out of bounds for axis 0 with size 657029"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    %time main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a627aa2-8277-45e6-ac9b-a0f491ff710d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:olx_otodom] *",
   "language": "python",
   "name": "conda-env-olx_otodom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
